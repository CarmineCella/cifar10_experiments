#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Deep Haar convolutional networks
\end_layout

\begin_layout Section
Setup
\end_layout

\begin_layout Standard
We set up a deep network for classification on CIFAR-10 using Tensorflow
 and Keras.
 In CIFAR-10 there are 10 classes, 50000 samples for training and 10000
 samples for testing.
 Images are 3x32x32.
 
\end_layout

\begin_layout Standard
All experiments have been done without optimizing hyperparameters, and using
 the following standard configuration:
\end_layout

\begin_layout Itemize
batch size = 128;
\end_layout

\begin_layout Itemize
For Keras: SGD with learning rate 0.01, (nesterov) momentum .9; For tensorflow:
 ADAM
\end_layout

\begin_layout Itemize
non linearity is always ReLU;
\end_layout

\begin_layout Itemize
standard data augmentation;
\end_layout

\begin_layout Itemize
100/150 epochs for training.
\end_layout

\begin_layout Section
Models
\end_layout

\begin_layout Standard
The general idea is to build models that construct convolutional invariants
 (covariants) along new axes, in an attempt to exhibit group structure at
 each extra axis.
 
\end_layout

\begin_layout Standard
A way to do this is to apply a multidimensional Haar transform that creates
 a new axis each layer and then recombine the features along the new axis
 linearly (linear combination of channels, aka 1x1 convolution, called 
\emph on
channel mix
\emph default
er).
\end_layout

\begin_layout Standard
In principle, several variants are possible for each block layer:
\end_layout

\begin_layout Itemize
Haar 
\begin_inset Formula $\rightarrow$
\end_inset

 channel mixer 
\begin_inset Formula $\rightarrow$
\end_inset

 batchnorm 
\begin_inset Formula $\rightarrow$
\end_inset

 relu ....
 ;
\end_layout

\begin_layout Itemize
spatial convolution + previous Haar model;
\end_layout

\begin_layout Itemize
spatial convolution + Haar only on discovered dimensions.
\end_layout

\begin_layout Standard
Several minor options are also available, such as: using or not a fully
  connected layer at the end and, adding one or more 1x1 convolutional layers
 per block (perceptron on channels, see network in network paper), using
 or not batch normalization and so on.
\end_layout

\begin_layout Section
Experiments with TensorFlow
\end_layout

\begin_layout Standard
In order to make the experiments possible we implemented a Haar transform
 and a mixing convolution (channel mixer) as TensorFlow components.
\end_layout

\begin_layout Standard
For the experiments we had a reference value of 92% of accuracy with an
 all convolutional model (Edouard-Tomas) in Tensorflow.
\end_layout

\begin_layout Standard
The results for the main experiments performed are reported below:
\end_layout

\begin_layout Itemize
Sergey (without last layer): 76% accuracy;
\end_layout

\begin_layout Itemize
H3 + FC: 69% accuracy;
\end_layout

\begin_layout Itemize
H4 + FC: 71% accuracy.
\end_layout

\begin_layout Standard
Here H3 means three block-layers as described above.
 There is no substantial change from H3 to H4.
 Sergey's model didn't achieve state of the art, maybe because there are
 hidden parameters that are handled differently in Torch (original implementatio
n) and TensorFlow; we didn't spend to much time on it, knowing that Tomas
 managed to have good results.
\end_layout

\begin_layout Subsection
Major issues
\end_layout

\begin_layout Itemize
Tensorflow limits tensor dimension to 5 for backpropagation; this seriously
 limited our capability to go deep in the network; (Keras solved this)
\end_layout

\begin_layout Itemize
difficult way of building networks in Tensorflow (most of the effort in
 Tensorflow and not in the models); (excessive overhead due to the fact
 that we are trying something  'non conventional')
\end_layout

\begin_layout Itemize
small number of parameters for Haar compared to fully connected; this would
 be an advantage if performance were equal, but may be a restriction at
 the moment;
\end_layout

\begin_layout Itemize
in space Haar doesn't really work, possibly due to the 2x2 filter footprint.
 Make it larger?
\end_layout

\begin_layout Standard
Implementing Haar transform in multiple dimensions using reshaped tensors
 with less dimensions should be possbile but is a major development effort;
 we decided to write the code in Keras to achieve effective results in less
 time.
\end_layout

\begin_layout Section
Experiments with Keras
\end_layout

\begin_layout Standard
We rewrote the Haar transform and the channel mixer as Keras components;
 this let us implement deeper networks without being limited by the size
 of the tensors.
 We performed the following experiments:
\end_layout

\begin_layout Itemize
spatial convolution + H5 (all conv): 62% accuracy;
\end_layout

\begin_layout Itemize
spatial convolution + H5 with double mixers (all conv): 63% accuracy
\end_layout

\begin_layout Itemize
H3 + 2FC (1024, 2048, 10): 71% accuracy;
\end_layout

\begin_layout Itemize
H3 + extra channel mixer at layer 2 + FC: 75% accuracy;
\end_layout

\begin_layout Itemize
spatial convolution + H2 + FC: 
\series bold
80%
\series default
 accuracy
\series bold
;
\end_layout

\begin_layout Itemize
spatial convolution + H2 with double mixers + FC: 
\series bold
80%
\series default
 accuracy
\end_layout

\begin_layout Subsection
Remarks
\end_layout

\begin_layout Standard
In these latest experiments with Keras, the loss curves are pretty smooth
 without flattening too much; this may mean that we just needed more epochs
 to converge.
 The Haar transform didn't really make the difference compared to the standard
 convolution or the fully connected layers, but maybe can be used to exploit
 some structure.
 In Keras, we did not focus on updating the learning rate/optimizing any
 other hyperparameters
\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
